<!DOCTYPE html><html lang="en"><head><link rel="preconnect" href="https://fonts.gstatic.com" crossorigin /><link rel="preload" as="style" data-href="https://fonts.googleapis.com/css2?family=Poppins:wght@200;300;400;500;600;700&amp;display=swap" data-optimized-fonts="true"/><meta charSet="UTF-8"/><meta name="viewport" content="width=device-width,initial-scale=1"/><meta property="og:title" content="Jina AI is a Neural Search Company"/><meta name="author" content="Jina AI"/><meta property="og:locale" content="en_US"/><meta property="og:site_name" content="Jina AI"/><meta property="og:image" content="https://jina.ai/assets/images/jina_banner_new.png"/><meta property="og:type" content="website"/><meta property="og:description" content="We provide business and developers an opensource neural search ecosystem for understanding unstructured data faster and easier."/><meta name="twitter:card" content="summary_large_image"/><meta property="twitter:image" content="https://jina.ai/assets/images/jina_banner_new.png"/><meta property="twitter:title" content="Jina AI is a Neural Search Company"/><meta property="twitter:description" content="We provide business and developers an opensource neural search ecosystem for understanding unstructured data faster and easier."/><meta name="twitter:site" content="@JinaAI_"/><meta name="twitter:creator" content="@JinaAI_"/><link rel="apple-touch-icon" href="/apple-touch-icon.png"/><link rel="icon" type="image/png" sizes="16x16" href="/favicon-16x16.png"/><link rel="icon" type="image/png" sizes="32x32" href="/favicon-32x32.png"/><link rel="icon" type="image/png" sizes="16x16" href="/favicon-96x96.png"/><link rel="icon" href="/favicon.ico"/><link rel="icon" type="image/png" sizes="192x192" href="/android-icon-192x192.png"/><link rel="stylesheet" data-href="https://fonts.googleapis.com/css2?family=Poppins:wght@200;300;400;500;600;700&amp;display=swap" data-optimized-fonts="true"/><meta name="robots" content="index,follow"/><meta name="googlebot" content="index,follow"/><meta name="description" content="Open source neural search ecosystem for businesses and developers, allowing anyone to search any kind of data with high availability and scalability."/><meta property="og:url" content="https://jina.ai/"/><meta property="og:title" content="Jina AI | Jina AI is a Neural Search Company"/><meta property="og:description" content="Open source neural search ecosystem for businesses and developers, allowing anyone to search any kind of data with high availability and scalability."/><meta property="og:locale" content="en"/><meta property="og:site_name" content="Jina AI, Ltd"/><link rel="canonical" href="https://jina.ai/"/><title>Building In-Video Visual Content Search with Jina | Jina AI</title><meta name="next-head-count" content="33"/><link rel="preload" href="/_next/static/css/e752e0fd702b7c34a608.css" as="style"/><link rel="stylesheet" href="/_next/static/css/e752e0fd702b7c34a608.css" data-n-g=""/><noscript data-n-css=""></noscript><script defer="" nomodule="" src="/_next/static/chunks/polyfills-a40ef1678bae11e696dba45124eadd70.js"></script><script src="/_next/static/chunks/webpack-f47d69457824065d04c3.js" defer=""></script><script src="/_next/static/chunks/framework-2191d16384373197bc0a.js" defer=""></script><script src="/_next/static/chunks/main-18f21ab86863c15f7e5d.js" defer=""></script><script src="/_next/static/chunks/pages/_app-25c384a6dfe2e0ee232e.js" defer=""></script><script src="/_next/static/chunks/419-571d18b1d9b4fc2cda27.js" defer=""></script><script src="/_next/static/chunks/522-f41528aa8639e5478b36.js" defer=""></script><script src="/_next/static/chunks/100-3a8cb6004ce37f1c54f0.js" defer=""></script><script src="/_next/static/chunks/pages/blog/%5Bslug%5D-b044910cba761441deff.js" defer=""></script><script src="/_next/static/c5lfS3RD0DKmWm5c4ajJe/_buildManifest.js" defer=""></script><script src="/_next/static/c5lfS3RD0DKmWm5c4ajJe/_ssgManifest.js" defer=""></script><style id="__jsx-3128403005">.dropdown.jsx-3128403005{--tw-bg-opacity:1;background-color:rgba(255,255,255,var(--tw-bg-opacity));--tw-text-opacity:1;color:rgba(74,85,104,var(--tw-text-opacity));top:5rem;width:100%;border-radius:0.75rem;}.dropdown.jsx-3128403005>.jsx-3128403005:not([hidden])~.jsx-3128403005:not([hidden]){--tw-divide-y-reverse:0;border-top-width:calc(1px * calc(1 - var(--tw-divide-y-reverse)));border-bottom-width:calc(1px * var(--tw-divide-y-reverse));border-style:solid;--tw-divide-opacity:1;border-color:rgba(235,235,235,var(--tw-divide-opacity));}</style><style id="__jsx-1573896879">.navbar.jsx-1573896879 a{display:inline-block;width:100%;}.navbar.jsx-1573896879 li:not(:first-child){margin-top:0.75rem;}.navbar.jsx-1573896879 a .btn{width:100%;}.navbar.jsx-1573896879 a:hover{--tw-text-opacity:1;color:rgba(0,129,129,var(--tw-text-opacity));}@media (min-width:640px){.navbar.jsx-1573896879 a,.navbar.jsx-1573896879 a .btn{width:auto;}.navbar.jsx-1573896879 li:not(:first-child){margin-top:0px;}.navbar.jsx-1573896879>li.jsx-1573896879:not(:last-child){margin-right:1.25rem;}}</style><style id="__jsx-649517993">.top-nav-bar.jsx-649517993{box-shadow:3px 6px 33px 0px #cdcdcd40;z-index:200;position:fixed;width:100vw;background:white;top:0;}</style><style id="__jsx-1324436386">.footer-links.jsx-1324436386 li{margin-top:0.25rem;}.footer-items-title.jsx-1324436386{font-size:1.375rem;line-height:1.5rem;-webkit-letter-spacing:0.22px;-moz-letter-spacing:0.22px;-ms-letter-spacing:0.22px;letter-spacing:0.22px;}</style><style id="__jsx-1956813867">.footer-left-margin.jsx-1956813867{width:20rem;height:inherit;background:no-repeat 40% 20%/30% url("/assets/images/planet-beige.svg");}.footer-right-margin.jsx-1956813867{width:16rem;height:inherit;background:no-repeat 10% 10%/50% url("/assets/images/planet-green.svg");}</style><style data-href="https://fonts.googleapis.com/css2?family=Poppins:wght@200;300;400;500;600;700&display=swap">@font-face{font-family:'Poppins';font-style:normal;font-weight:200;font-display:swap;src:url(https://fonts.gstatic.com/s/poppins/v15/pxiByp8kv8JHgFVrLFj_V1g.woff) format('woff')}@font-face{font-family:'Poppins';font-style:normal;font-weight:300;font-display:swap;src:url(https://fonts.gstatic.com/s/poppins/v15/pxiByp8kv8JHgFVrLDz8V1g.woff) format('woff')}@font-face{font-family:'Poppins';font-style:normal;font-weight:400;font-display:swap;src:url(https://fonts.gstatic.com/s/poppins/v15/pxiEyp8kv8JHgFVrFJM.woff) format('woff')}@font-face{font-family:'Poppins';font-style:normal;font-weight:500;font-display:swap;src:url(https://fonts.gstatic.com/s/poppins/v15/pxiByp8kv8JHgFVrLGT9V1g.woff) format('woff')}@font-face{font-family:'Poppins';font-style:normal;font-weight:600;font-display:swap;src:url(https://fonts.gstatic.com/s/poppins/v15/pxiByp8kv8JHgFVrLEj6V1g.woff) format('woff')}@font-face{font-family:'Poppins';font-style:normal;font-weight:700;font-display:swap;src:url(https://fonts.gstatic.com/s/poppins/v15/pxiByp8kv8JHgFVrLCz7V1g.woff) format('woff')}@font-face{font-family:'Poppins';font-style:normal;font-weight:200;font-display:swap;src:url(https://fonts.gstatic.com/s/poppins/v15/pxiByp8kv8JHgFVrLFj_Z11lFd2JQEl8qw.woff2) format('woff2');unicode-range:U+0900-097F,U+1CD0-1CF6,U+1CF8-1CF9,U+200C-200D,U+20A8,U+20B9,U+25CC,U+A830-A839,U+A8E0-A8FB}@font-face{font-family:'Poppins';font-style:normal;font-weight:200;font-display:swap;src:url(https://fonts.gstatic.com/s/poppins/v15/pxiByp8kv8JHgFVrLFj_Z1JlFd2JQEl8qw.woff2) format('woff2');unicode-range:U+0100-024F,U+0259,U+1E00-1EFF,U+2020,U+20A0-20AB,U+20AD-20CF,U+2113,U+2C60-2C7F,U+A720-A7FF}@font-face{font-family:'Poppins';font-style:normal;font-weight:200;font-display:swap;src:url(https://fonts.gstatic.com/s/poppins/v15/pxiByp8kv8JHgFVrLFj_Z1xlFd2JQEk.woff2) format('woff2');unicode-range:U+0000-00FF,U+0131,U+0152-0153,U+02BB-02BC,U+02C6,U+02DA,U+02DC,U+2000-206F,U+2074,U+20AC,U+2122,U+2191,U+2193,U+2212,U+2215,U+FEFF,U+FFFD}@font-face{font-family:'Poppins';font-style:normal;font-weight:300;font-display:swap;src:url(https://fonts.gstatic.com/s/poppins/v15/pxiByp8kv8JHgFVrLDz8Z11lFd2JQEl8qw.woff2) format('woff2');unicode-range:U+0900-097F,U+1CD0-1CF6,U+1CF8-1CF9,U+200C-200D,U+20A8,U+20B9,U+25CC,U+A830-A839,U+A8E0-A8FB}@font-face{font-family:'Poppins';font-style:normal;font-weight:300;font-display:swap;src:url(https://fonts.gstatic.com/s/poppins/v15/pxiByp8kv8JHgFVrLDz8Z1JlFd2JQEl8qw.woff2) format('woff2');unicode-range:U+0100-024F,U+0259,U+1E00-1EFF,U+2020,U+20A0-20AB,U+20AD-20CF,U+2113,U+2C60-2C7F,U+A720-A7FF}@font-face{font-family:'Poppins';font-style:normal;font-weight:300;font-display:swap;src:url(https://fonts.gstatic.com/s/poppins/v15/pxiByp8kv8JHgFVrLDz8Z1xlFd2JQEk.woff2) format('woff2');unicode-range:U+0000-00FF,U+0131,U+0152-0153,U+02BB-02BC,U+02C6,U+02DA,U+02DC,U+2000-206F,U+2074,U+20AC,U+2122,U+2191,U+2193,U+2212,U+2215,U+FEFF,U+FFFD}@font-face{font-family:'Poppins';font-style:normal;font-weight:400;font-display:swap;src:url(https://fonts.gstatic.com/s/poppins/v15/pxiEyp8kv8JHgFVrJJbecnFHGPezSQ.woff2) format('woff2');unicode-range:U+0900-097F,U+1CD0-1CF6,U+1CF8-1CF9,U+200C-200D,U+20A8,U+20B9,U+25CC,U+A830-A839,U+A8E0-A8FB}@font-face{font-family:'Poppins';font-style:normal;font-weight:400;font-display:swap;src:url(https://fonts.gstatic.com/s/poppins/v15/pxiEyp8kv8JHgFVrJJnecnFHGPezSQ.woff2) format('woff2');unicode-range:U+0100-024F,U+0259,U+1E00-1EFF,U+2020,U+20A0-20AB,U+20AD-20CF,U+2113,U+2C60-2C7F,U+A720-A7FF}@font-face{font-family:'Poppins';font-style:normal;font-weight:400;font-display:swap;src:url(https://fonts.gstatic.com/s/poppins/v15/pxiEyp8kv8JHgFVrJJfecnFHGPc.woff2) format('woff2');unicode-range:U+0000-00FF,U+0131,U+0152-0153,U+02BB-02BC,U+02C6,U+02DA,U+02DC,U+2000-206F,U+2074,U+20AC,U+2122,U+2191,U+2193,U+2212,U+2215,U+FEFF,U+FFFD}@font-face{font-family:'Poppins';font-style:normal;font-weight:500;font-display:swap;src:url(https://fonts.gstatic.com/s/poppins/v15/pxiByp8kv8JHgFVrLGT9Z11lFd2JQEl8qw.woff2) format('woff2');unicode-range:U+0900-097F,U+1CD0-1CF6,U+1CF8-1CF9,U+200C-200D,U+20A8,U+20B9,U+25CC,U+A830-A839,U+A8E0-A8FB}@font-face{font-family:'Poppins';font-style:normal;font-weight:500;font-display:swap;src:url(https://fonts.gstatic.com/s/poppins/v15/pxiByp8kv8JHgFVrLGT9Z1JlFd2JQEl8qw.woff2) format('woff2');unicode-range:U+0100-024F,U+0259,U+1E00-1EFF,U+2020,U+20A0-20AB,U+20AD-20CF,U+2113,U+2C60-2C7F,U+A720-A7FF}@font-face{font-family:'Poppins';font-style:normal;font-weight:500;font-display:swap;src:url(https://fonts.gstatic.com/s/poppins/v15/pxiByp8kv8JHgFVrLGT9Z1xlFd2JQEk.woff2) format('woff2');unicode-range:U+0000-00FF,U+0131,U+0152-0153,U+02BB-02BC,U+02C6,U+02DA,U+02DC,U+2000-206F,U+2074,U+20AC,U+2122,U+2191,U+2193,U+2212,U+2215,U+FEFF,U+FFFD}@font-face{font-family:'Poppins';font-style:normal;font-weight:600;font-display:swap;src:url(https://fonts.gstatic.com/s/poppins/v15/pxiByp8kv8JHgFVrLEj6Z11lFd2JQEl8qw.woff2) format('woff2');unicode-range:U+0900-097F,U+1CD0-1CF6,U+1CF8-1CF9,U+200C-200D,U+20A8,U+20B9,U+25CC,U+A830-A839,U+A8E0-A8FB}@font-face{font-family:'Poppins';font-style:normal;font-weight:600;font-display:swap;src:url(https://fonts.gstatic.com/s/poppins/v15/pxiByp8kv8JHgFVrLEj6Z1JlFd2JQEl8qw.woff2) format('woff2');unicode-range:U+0100-024F,U+0259,U+1E00-1EFF,U+2020,U+20A0-20AB,U+20AD-20CF,U+2113,U+2C60-2C7F,U+A720-A7FF}@font-face{font-family:'Poppins';font-style:normal;font-weight:600;font-display:swap;src:url(https://fonts.gstatic.com/s/poppins/v15/pxiByp8kv8JHgFVrLEj6Z1xlFd2JQEk.woff2) format('woff2');unicode-range:U+0000-00FF,U+0131,U+0152-0153,U+02BB-02BC,U+02C6,U+02DA,U+02DC,U+2000-206F,U+2074,U+20AC,U+2122,U+2191,U+2193,U+2212,U+2215,U+FEFF,U+FFFD}@font-face{font-family:'Poppins';font-style:normal;font-weight:700;font-display:swap;src:url(https://fonts.gstatic.com/s/poppins/v15/pxiByp8kv8JHgFVrLCz7Z11lFd2JQEl8qw.woff2) format('woff2');unicode-range:U+0900-097F,U+1CD0-1CF6,U+1CF8-1CF9,U+200C-200D,U+20A8,U+20B9,U+25CC,U+A830-A839,U+A8E0-A8FB}@font-face{font-family:'Poppins';font-style:normal;font-weight:700;font-display:swap;src:url(https://fonts.gstatic.com/s/poppins/v15/pxiByp8kv8JHgFVrLCz7Z1JlFd2JQEl8qw.woff2) format('woff2');unicode-range:U+0100-024F,U+0259,U+1E00-1EFF,U+2020,U+20A0-20AB,U+20AD-20CF,U+2113,U+2C60-2C7F,U+A720-A7FF}@font-face{font-family:'Poppins';font-style:normal;font-weight:700;font-display:swap;src:url(https://fonts.gstatic.com/s/poppins/v15/pxiByp8kv8JHgFVrLCz7Z1xlFd2JQEk.woff2) format('woff2');unicode-range:U+0000-00FF,U+0131,U+0152-0153,U+02BB-02BC,U+02C6,U+02DA,U+02DC,U+2000-206F,U+2074,U+20AC,U+2122,U+2191,U+2193,U+2212,U+2215,U+FEFF,U+FFFD}</style></head><script async="" src="https://www.googletagmanager.com/gtag/js?id=undefined"></script><script>
            window.dataLayer = window.dataLayer || [];
            function gtag(){dataLayer.push(arguments);}
            gtag('js', new Date());
            gtag('config', 'undefined', {
              page_path: window.location.pathname,
            });
          </script><body><div id="__next"><div class="jsx-649517993 top-nav-bar"><div class="md:max-w-screen-lg lg:max-w-screen-xl mx-auto px-8 py-6 undefined"><div class="jsx-1573896879 flex flex-wrap justify-between items-center"><div class="jsx-1573896879"><a class="jsx-1573896879" href="/"><div class="text-gray-900 flex items-center font-semibold text-3xl"><img src="/assets/images/logo.svg" alt="Jina.ai logo" class="w-24"/></div></a></div><div class="jsx-1573896879 md:hidden"><button type="button" aria-label="show menu" class="jsx-1573896879 p-3 text-gray-900 rounded-md hover:bg-white"><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24" stroke-width="1.5" fill="none" stroke-linecap="round" stroke-linejoin="round" class="jsx-1573896879 stroke-current h-6 w-6"><path d="M0 0h24v24H0z" stroke="none" class="jsx-1573896879"></path><path d="M4 6h16M4 12h16M4 18h16" class="jsx-1573896879"></path></svg></button></div><nav class="jsx-1573896879 w-full md:w-auto md:block mt-6 md:mt-0 hidden"><ul class="jsx-1573896879 navbar flex flex-col md:flex-row md:items-center font-medium text-xl text-gray-800 sm:p-0 bg-white sm:bg-transparent rounded"><a href="/careers" class="jsx-649517993"><span class="jsx-649517993 text-gray-700 text-lg mr-4 font-medium hidden md:inline-block">Careers</span></a><a href="/blog" class="jsx-649517993"><span class="jsx-649517993 text-gray-700 text-lg mr-4 font-medium hidden md:inline-block">Blog</span></a><a href="https://learn.jina.ai" class="jsx-649517993"><span class="jsx-649517993 text-gray-700 text-lg mr-4 font-medium hidden md:inline-block">Learn</span></a><div class="jsx-3128403005 md:hidden overflow-y-scroll"><ul class="jsx-3128403005 dropdown"><li class="jsx-3128403005 hover:bg-gray-200"><a href="/careers" rel="noopener noreferrer" class="jsx-3128403005 py-2 px-4 block whitespace-no-wrap"><div class="jsx-3128403005 leading-4"><div class="jsx-3128403005 flex items-center"><div class=""><img src="/assets/images/icons/careers-icon.svg" alt="careers icon" class="w-full h-full"/></div><a href="/careers" class="jsx-3128403005"><span class="jsx-3128403005 text-gray-800 font-semibold text-sm ml-2">Careers</span></a></div><span class="jsx-3128403005 text-xs text-gray-500">Interested in joining us? Find out what it&#x27;s like to work at Jina AI.</span></div></a></li><li class="jsx-3128403005 hover:bg-gray-200"><a href="/blog" rel="noopener noreferrer" class="jsx-3128403005 py-2 px-4 block whitespace-no-wrap"><div class="jsx-3128403005 leading-4"><div class="jsx-3128403005 flex items-center"><div class=""><img src="/assets/images/icons/news-icon.svg" alt="news icon" class="w-full h-full"/></div><a href="/blog" class="jsx-3128403005"><span class="jsx-3128403005 text-gray-800 font-semibold text-sm ml-2">Blog</span></a></div><span class="jsx-3128403005 text-xs text-gray-500">Check out the latest news about our company and products.</span></div></a></li><li class="jsx-3128403005 hover:bg-gray-200"><a href="https://learn.jina.ai" rel="noopener noreferrer" class="jsx-3128403005 py-2 px-4 block whitespace-no-wrap"><div class="jsx-3128403005 leading-4"><div class="jsx-3128403005 flex items-center"><div class=""><img src="/assets/images/icons/docs-icon.svg" alt="docs icon" class="w-full h-full"/></div><a href="https://learn.jina.ai" class="jsx-3128403005"><span class="jsx-3128403005 text-gray-800 font-semibold text-sm ml-2">Learn</span></a></div><span class="jsx-3128403005 text-xs text-gray-500">Jina Learning Bootcamp</span></div></a></li></ul></div></ul></nav></div></div></div><div style="margin-top:5.5rem"><div class="min-h-screen"><main><div class="container mx-auto px-5 flex flex-col items-center"><img src="/assets/images/blog/2021-12-15_in-video_visual_content/banner.png" alt="Building In-Video Visual Content Search with Jina-image" class="rounded-lg md:rounded-none shadow-md md:shadow-lg mb-4 mt-8 md:my-0 w-full xl:rounded-b-3xl"/><article class="mb-16 md:mb-32 pt-5 pb-10 md:py-20 md:max-w-6xl rounded-b-lg xl:rounded-3xl md:px-32 w-full xl:-mt-20 bg-white md:shadow-lg"><div class="mb-3"><div class="inline-block px-2 py-0 rounded text-primary-500 bg-primary-500 bg-opacity-20 mr-2 mb-2">video</div><div class="inline-block px-2 py-0 rounded text-primary-500 bg-primary-500 bg-opacity-20 mr-2 mb-2">search</div></div><h1 class="text-xl md:text-4xl font-bold tracking-tighter leading-tight md:leading-none mb-6 md:text-left">Building In-Video Visual Content Search with Jina</h1><div class="flex items-center justify-between text-gray-600 mb-12"><div class="flex items-center"><img src="/assets/images/team/avatar-default.svg" class="w-8 h-auto rounded-full border border-gray-500 mr-4" alt="Nan Wang"/><div class="text-gray-600">Nan Wang</div></div><time dateTime="2021-12-15">15 December, 2021</time></div><div class="mx-auto markdown"><h2 id="overview">Overview</h2>
<p>Videos are one of the most popular ways to consume data today. Whether it's live streams of our favourite music artists or recorded tutorial videos, we watch them all.</p>
<p>Perhaps you came across a music video of your favourite artist while browsing YouTube. However, you can't recall the video's name, title, or song. In that case, you only have an image in mind; that image can be of the artist holding a guitar and singing.</p>
<p>Now you can just explain the image scenario to Youtube’s search engine, and it will magically come up with the resulting video. Today’s search systems are intelligent enough to retrieve data with just a hint of information. Even if you enter <em>“XYZ holding a guitar”</em>, Youtube's search powered by state-of-the-art deep learning models will be able to come up with some results.</p>
<p>Now imagine you having the capability to create such robust search systems for your internal applications, and that too in a matter of hours! But how would you do that?</p>
<p>That’s where <a href="https://hub.jina.ai/"><strong>Jina Hub</strong></a> comes in! It lets you use the best open-source models with a single line of code, and that combined with the core Jina framework allows you to create magic!</p>
<p>In this blog post, we will see how to create a search system capable of searching the in-video content without supplement text.</p>
<p class="image-only"><span class="image-wrapper"><img src="/assets/images/blog/2021-12-15_in-video_visual_content/visual_search_interface.gif" alt=""></span></p>
<p align="center">
Fig 1. In-video Visual Search Interface
</p>
<h2 id="flow-approach">Flow Approach</h2>
<p>We don’t have any textual information about the video in this example. So we cannot match the query directly with the text information of the video in the form of subtitles. We need to find a way of matching text to videos. We know that videos are made of frames/images arranged in a sequential order to form a video. Using this concept, we can build our use case. We can find related frames similar to the query text and return the videos containing these frames as output.</p>
<p>Normally, every video comprises of three components - audio, video and text. In this example, we will work only with images. If video consists of only text or a static image forming one unique frame, then our use case would not work as we are leveraging the movement of those frames. Also, one more shortcoming of this tutorial is the ability to extend to multiple frames. It means that if you enter <em>“XYZ holding a guitar and then signing an autograph for a boy in a white t-shirt”</em>, this search query cannot be captured in a single frame and hence, is beyond the scope of this tutorial.</p>
<p>We want a deep learning model that can encode both query text and video frames in the same semantic space for our use case. Therefore, we will use pre-trained cross-modal models from <a href="https://hub.jina.ai/"><strong><strong>Jina Hub</strong></strong></a>!</p>
<h3 id="executors-from-jina-hub">Executors from Jina Hub</h3>
<p>To encode video frames and query text into the same space, we will use the pre-trained <a href="https://github.com/openai/CLIP">CLIP Model</a> from OpenAI. We will use the image and text encoding part of the CLIP model to calculate the embeddings for this example application.</p>
<p><strong>What is CLIP?</strong></p>
<p>CLIP stands for Contrastive Language-Image Pre-Training. It is trained to learn visual concepts from natural languages with the help of text snippets and image pairs from the internet. It can perform Zero-Shot Learning by encoding text labels and images in the same semantic space and creating the standard embedding for both modalities.</p>
<p><strong>Why CLIP?</strong></p>
<p>CLIP works very well for our use case of searching for video content. Let’s say we enter the search text <em>"this is a guitar"</em>, the CLIP text model will encode it into a vector. Similarly, the CLIP image model can encode an image of a guitar and a violin into the same vector space. Encoding both the text and images into the same space allows us to calculate the distance between the text vector and the image vector to provide relevant results. In this example, the distance between the text <em>"this is a guitar"</em>, and the image of the guitar will be smaller than the distance between the same text and the image of the violin.</p>
<p>For this example, we will use SimpleIndexer as our indexer as it allows us to store both vectors and meta-data information in one shot. For searching through the indexed data, we will use the built-in <code>match</code> function of <code>DocumentArrayMemap</code>.</p>
<p class="image-only"><span class="image-wrapper"><img src="/assets/images/blog/2021-12-15_in-video_visual_content/working_of_clip.svg" alt=""></span></p>
<p align="center">
Fig 2. Working of CLIP Model
</p>
<h3 id="building-the-flow">Building the Flow</h3>
<p>Let’s go through each of the steps in the Flow in sequential order to understand what’s happening behind the scenes:</p>
<ul>
<li><code>frame_extractor</code>: It extracts the frames from the videos allowing us to work with the image data type encoded in the same space as the query text.</li>
<li><code>image_encoder</code>: It uses the CLIP image encoder to encode the extracted frames into the common vector space.</li>
<li><code>text_encoder</code>: It uses the CLIP text encoder to encode the query text into the same vector space where the frames are encoded.</li>
<li><code>indexer</code>: It uses SimpleIndexer to index the encoded text and image data for querying</li>
<li><code>ranker</code>: It ranks the query results based on the degree of similarity in the vector space.</li>
</ul>
<p class="image-only"><span class="image-wrapper"><img src="/assets/images/blog/2021-12-15_in-video_visual_content/application_flow.svg" alt=""></span></p>
<p align="center">
Fig 3. Application Flow
</p>
<p>We have seen how the different Flow components work together to process the query text and generate the response. Now, let’s understand the two types of requests in detail - <code>index</code> and <code>query</code>.</p>
<h3 id="index">Index</h3>
<p>For requests to the <code>/index</code> endpoint, the indexing flow uses three different Executors - <code>VideoLoader</code>, <code>CLIPImageEncoder</code> and <code>SimpleIndexer</code> to pre-process and index the data. It follows a sequential flow of data as discussed in the below steps:</p>
<ul>
<li>The input to Flow is Documents with video URIs stored in the <code>uri</code> attribute. These can be files on the cloud or your local machine. After receiving the raw input, control goes to the <code>VideoLoader</code> that extracts the frames from the videos and stores them as image arrays in the <code>blob</code> attribute of the chunks.</li>
<li>The processed frames are passed onto the <code>CLIPImageEncoder</code> that calculates the 512-dimensional embedding vector for each chunk using the CLIP model for images.</li>
<li>Finally, the control is passed to <code>SimpleIndexer</code> that stores and indexes all the Documents within the memory map.</li>
</ul>
<h3 id="query">Query</h3>
<p>For requests to the <code>/search</code> endpoint, also known as query endpoint, the query flow uses three different Executors - <code>CLIPTextEncoder</code>, <code>SimpleIndexe</code>r, and <code>SimpleRanker</code> to pre-process the query text and match it with the indexed data. It follows a sequential flow of data as discussed in the below steps:</p>
<ul>
<li>The user input gets stored in the <code>text</code> attribute of the Document. After that, the control goes to <code>CLIPTextEncoder</code>, which converts the query text into vector embedding.</li>
<li>After getting the embeddings for the search query, the <code>SimpleIndexer</code> compares the query embedding vector with the indexed data to retrieve the top-K nearest neighbours.</li>
<li>In the end, the control goes to the <code>SimpleRanker</code> that ranks the results and shows the most relevant ones.</li>
</ul>
<blockquote>
<p><strong>Tips</strong>: Find more information at Jina Hub about <a href="https://hub.jina.ai/executor/livtkbkg">CLIPTextEncoder</a>, <a href="https://hub.jina.ai/executor/0hnlmu3q">CLIPImageEncoder</a> and <a href="https://hub.jina.ai/executor/zb38xlt4">SimpleIndexer</a>.</p>
</blockquote>
<h2 id="summary">Summary</h2>
<p>In this blog, we learned how to create an intelligent in-video visual content search system by leveraging state-of-the-art opensource models with Jina’s framework. This use case can further be extended to incorporate audio data and video frames to improve the quality of search results. We can use the <a href="https://github.com/AndreyGuzhov/AudioCLIP">AudioCLIP</a> model from OpenAI to generate embeddings for audio in the same semantic space as images and text.</p>
<p>You can find the application code in the following <a href="https://github.com/jina-ai/example-video-search/tree/feat-simple-tutorial">GitHub Repository</a></p>
<p>In the future posts, we will cover more about building SOTA search applications by leveraging Jina Hub. Stay tuned and happy Searching!</p></div></article></div></main></div></div><div class="jsx-1956813867"><div class="bg-primary-500 text-white"><div class="jsx-1956813867 flex flex-row justify-center md:justify-between px-6 md:px-0"><div class="jsx-1956813867 footer-left-margin hidden md:block"></div><div class="jsx-1956813867 flex justify-center flex-col md:flex-row"><div class="jsx-1956813867 pr-4 md:ml-0 md:py-16 md:mr-2 lg:mr-16 mt-16 md:mt-0"><div class="text-center md:text-left"><div class="flex md:justify-start"><div class="text-gray-900 flex items-center font-semibold text-3xl"><img src="/assets/images/logo-white.svg" alt="Jina.ai logo" class="w-24"/></div></div></div></div><div class="jsx-1956813867 grid grid-cols-2 sm:grid-cols-2 md:grid-cols-4 gap-16 py-16"><div class="jsx-1324436386 footer-links sm:text-left"><div class="jsx-1324436386 font-semibold text-white footer-items-title">Company</div><nav class="jsx-1324436386 mt-3"><ul class="jsx-1324436386"><li class="jsx-1956813867"><a class="jsx-1956813867" href="/careers/">Join us</a></li><li class="jsx-1956813867"><a class="jsx-1956813867" href="/blog/">Blog</a></li><li class="jsx-1956813867"><a class="jsx-1956813867" href="https://learn.jina.ai">Learn</a></li></ul></nav></div><div class="jsx-1324436386 footer-links sm:text-left"><div class="jsx-1324436386 font-semibold text-white footer-items-title">Products</div><nav class="jsx-1324436386 mt-3"><ul class="jsx-1324436386"><li class="jsx-1956813867"><a class="jsx-1956813867" href="https://github.com/jina-ai/jina">Jina</a></li><li class="jsx-1956813867"><a class="jsx-1956813867" href="https://hub.jina.ai">Hub</a></li><li class="jsx-1956813867"><a class="jsx-1956813867" href="https://github.com/jina-ai/finetuner">Finetuner</a></li></ul></nav></div><div class="jsx-1324436386 footer-links sm:text-left"><div class="jsx-1324436386 font-semibold text-white footer-items-title">Legal</div><nav class="jsx-1324436386 mt-3"><ul class="jsx-1324436386"><li class="jsx-1956813867"><a class="jsx-1956813867" href="/legal/#terms-of-service">Terms of Service</a></li><li class="jsx-1956813867"><a class="jsx-1956813867" href="/legal/#privacy-policy">Privacy Policy</a></li></ul></nav></div><div class="jsx-1324436386 footer-links sm:text-left"><div class="jsx-1324436386 font-semibold text-white footer-items-title">Social</div><nav class="jsx-1324436386 mt-3"><ul class="jsx-1324436386"><li class="jsx-1956813867"><a class="jsx-1956813867" href="https://github.com/jina-ai/">GitHub</a></li><li class="jsx-1956813867"><a class="jsx-1956813867" href="https://www.linkedin.com/company/jinaai/">LinkedIn</a></li><li class="jsx-1956813867"><a class="jsx-1956813867" href="https://twitter.com/jinaAI_/">Twitter</a></li><li class="jsx-1956813867"><a class="jsx-1956813867" href="https://www.youtube.com/c/JinaAI/">YouTube</a></li><li class="jsx-1956813867"><a class="jsx-1956813867" href="https://slack.jina.ai/">Slack</a></li><li class="jsx-1956813867"><a class="jsx-1956813867" href="https://meetup.com/jina-community-meetup/">Meetup</a></li></ul></nav></div></div></div><div class="jsx-1956813867 footer-right-margin hidden md:block"></div></div><div class="text-center text-gray-100 text-sm py-2"><span class="block md:inline-block">© Jina AI 2020-2021. All rights reserved.</span></div></div></div></div><script id="__NEXT_DATA__" type="application/json">{"props":{"pageProps":{"post":{"title":"Building In-Video Visual Content Search with Jina","date":"2021-12-15","slug":"2021-12-15_in-video_visual_content","author":"Nan Wang","content":"\u003ch2 id=\"overview\"\u003eOverview\u003c/h2\u003e\n\u003cp\u003eVideos are one of the most popular ways to consume data today. Whether it's live streams of our favourite music artists or recorded tutorial videos, we watch them all.\u003c/p\u003e\n\u003cp\u003ePerhaps you came across a music video of your favourite artist while browsing YouTube. However, you can't recall the video's name, title, or song. In that case, you only have an image in mind; that image can be of the artist holding a guitar and singing.\u003c/p\u003e\n\u003cp\u003eNow you can just explain the image scenario to Youtube’s search engine, and it will magically come up with the resulting video. Today’s search systems are intelligent enough to retrieve data with just a hint of information. Even if you enter \u003cem\u003e“XYZ holding a guitar”\u003c/em\u003e, Youtube's search powered by state-of-the-art deep learning models will be able to come up with some results.\u003c/p\u003e\n\u003cp\u003eNow imagine you having the capability to create such robust search systems for your internal applications, and that too in a matter of hours! But how would you do that?\u003c/p\u003e\n\u003cp\u003eThat’s where \u003ca href=\"https://hub.jina.ai/\"\u003e\u003cstrong\u003eJina Hub\u003c/strong\u003e\u003c/a\u003e comes in! It lets you use the best open-source models with a single line of code, and that combined with the core Jina framework allows you to create magic!\u003c/p\u003e\n\u003cp\u003eIn this blog post, we will see how to create a search system capable of searching the in-video content without supplement text.\u003c/p\u003e\n\u003cp class=\"image-only\"\u003e\u003cspan class=\"image-wrapper\"\u003e\u003cimg src=\"/assets/images/blog/2021-12-15_in-video_visual_content/visual_search_interface.gif\" alt=\"\"\u003e\u003c/span\u003e\u003c/p\u003e\n\u003cp align=\"center\"\u003e\nFig 1. In-video Visual Search Interface\n\u003c/p\u003e\n\u003ch2 id=\"flow-approach\"\u003eFlow Approach\u003c/h2\u003e\n\u003cp\u003eWe don’t have any textual information about the video in this example. So we cannot match the query directly with the text information of the video in the form of subtitles. We need to find a way of matching text to videos. We know that videos are made of frames/images arranged in a sequential order to form a video. Using this concept, we can build our use case. We can find related frames similar to the query text and return the videos containing these frames as output.\u003c/p\u003e\n\u003cp\u003eNormally, every video comprises of three components - audio, video and text. In this example, we will work only with images. If video consists of only text or a static image forming one unique frame, then our use case would not work as we are leveraging the movement of those frames. Also, one more shortcoming of this tutorial is the ability to extend to multiple frames. It means that if you enter \u003cem\u003e“XYZ holding a guitar and then signing an autograph for a boy in a white t-shirt”\u003c/em\u003e, this search query cannot be captured in a single frame and hence, is beyond the scope of this tutorial.\u003c/p\u003e\n\u003cp\u003eWe want a deep learning model that can encode both query text and video frames in the same semantic space for our use case. Therefore, we will use pre-trained cross-modal models from \u003ca href=\"https://hub.jina.ai/\"\u003e\u003cstrong\u003e\u003cstrong\u003eJina Hub\u003c/strong\u003e\u003c/strong\u003e\u003c/a\u003e!\u003c/p\u003e\n\u003ch3 id=\"executors-from-jina-hub\"\u003eExecutors from Jina Hub\u003c/h3\u003e\n\u003cp\u003eTo encode video frames and query text into the same space, we will use the pre-trained \u003ca href=\"https://github.com/openai/CLIP\"\u003eCLIP Model\u003c/a\u003e from OpenAI. We will use the image and text encoding part of the CLIP model to calculate the embeddings for this example application.\u003c/p\u003e\n\u003cp\u003e\u003cstrong\u003eWhat is CLIP?\u003c/strong\u003e\u003c/p\u003e\n\u003cp\u003eCLIP stands for Contrastive Language-Image Pre-Training. It is trained to learn visual concepts from natural languages with the help of text snippets and image pairs from the internet. It can perform Zero-Shot Learning by encoding text labels and images in the same semantic space and creating the standard embedding for both modalities.\u003c/p\u003e\n\u003cp\u003e\u003cstrong\u003eWhy CLIP?\u003c/strong\u003e\u003c/p\u003e\n\u003cp\u003eCLIP works very well for our use case of searching for video content. Let’s say we enter the search text \u003cem\u003e\"this is a guitar\"\u003c/em\u003e, the CLIP text model will encode it into a vector. Similarly, the CLIP image model can encode an image of a guitar and a violin into the same vector space. Encoding both the text and images into the same space allows us to calculate the distance between the text vector and the image vector to provide relevant results. In this example, the distance between the text \u003cem\u003e\"this is a guitar\"\u003c/em\u003e, and the image of the guitar will be smaller than the distance between the same text and the image of the violin.\u003c/p\u003e\n\u003cp\u003eFor this example, we will use SimpleIndexer as our indexer as it allows us to store both vectors and meta-data information in one shot. For searching through the indexed data, we will use the built-in \u003ccode\u003ematch\u003c/code\u003e function of \u003ccode\u003eDocumentArrayMemap\u003c/code\u003e.\u003c/p\u003e\n\u003cp class=\"image-only\"\u003e\u003cspan class=\"image-wrapper\"\u003e\u003cimg src=\"/assets/images/blog/2021-12-15_in-video_visual_content/working_of_clip.svg\" alt=\"\"\u003e\u003c/span\u003e\u003c/p\u003e\n\u003cp align=\"center\"\u003e\nFig 2. Working of CLIP Model\n\u003c/p\u003e\n\u003ch3 id=\"building-the-flow\"\u003eBuilding the Flow\u003c/h3\u003e\n\u003cp\u003eLet’s go through each of the steps in the Flow in sequential order to understand what’s happening behind the scenes:\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e\u003ccode\u003eframe_extractor\u003c/code\u003e: It extracts the frames from the videos allowing us to work with the image data type encoded in the same space as the query text.\u003c/li\u003e\n\u003cli\u003e\u003ccode\u003eimage_encoder\u003c/code\u003e: It uses the CLIP image encoder to encode the extracted frames into the common vector space.\u003c/li\u003e\n\u003cli\u003e\u003ccode\u003etext_encoder\u003c/code\u003e: It uses the CLIP text encoder to encode the query text into the same vector space where the frames are encoded.\u003c/li\u003e\n\u003cli\u003e\u003ccode\u003eindexer\u003c/code\u003e: It uses SimpleIndexer to index the encoded text and image data for querying\u003c/li\u003e\n\u003cli\u003e\u003ccode\u003eranker\u003c/code\u003e: It ranks the query results based on the degree of similarity in the vector space.\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp class=\"image-only\"\u003e\u003cspan class=\"image-wrapper\"\u003e\u003cimg src=\"/assets/images/blog/2021-12-15_in-video_visual_content/application_flow.svg\" alt=\"\"\u003e\u003c/span\u003e\u003c/p\u003e\n\u003cp align=\"center\"\u003e\nFig 3. Application Flow\n\u003c/p\u003e\n\u003cp\u003eWe have seen how the different Flow components work together to process the query text and generate the response. Now, let’s understand the two types of requests in detail - \u003ccode\u003eindex\u003c/code\u003e and \u003ccode\u003equery\u003c/code\u003e.\u003c/p\u003e\n\u003ch3 id=\"index\"\u003eIndex\u003c/h3\u003e\n\u003cp\u003eFor requests to the \u003ccode\u003e/index\u003c/code\u003e endpoint, the indexing flow uses three different Executors - \u003ccode\u003eVideoLoader\u003c/code\u003e, \u003ccode\u003eCLIPImageEncoder\u003c/code\u003e and \u003ccode\u003eSimpleIndexer\u003c/code\u003e to pre-process and index the data. It follows a sequential flow of data as discussed in the below steps:\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003eThe input to Flow is Documents with video URIs stored in the \u003ccode\u003euri\u003c/code\u003e attribute. These can be files on the cloud or your local machine. After receiving the raw input, control goes to the \u003ccode\u003eVideoLoader\u003c/code\u003e that extracts the frames from the videos and stores them as image arrays in the \u003ccode\u003eblob\u003c/code\u003e attribute of the chunks.\u003c/li\u003e\n\u003cli\u003eThe processed frames are passed onto the \u003ccode\u003eCLIPImageEncoder\u003c/code\u003e that calculates the 512-dimensional embedding vector for each chunk using the CLIP model for images.\u003c/li\u003e\n\u003cli\u003eFinally, the control is passed to \u003ccode\u003eSimpleIndexer\u003c/code\u003e that stores and indexes all the Documents within the memory map.\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch3 id=\"query\"\u003eQuery\u003c/h3\u003e\n\u003cp\u003eFor requests to the \u003ccode\u003e/search\u003c/code\u003e endpoint, also known as query endpoint, the query flow uses three different Executors - \u003ccode\u003eCLIPTextEncoder\u003c/code\u003e, \u003ccode\u003eSimpleIndexe\u003c/code\u003er, and \u003ccode\u003eSimpleRanker\u003c/code\u003e to pre-process the query text and match it with the indexed data. It follows a sequential flow of data as discussed in the below steps:\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003eThe user input gets stored in the \u003ccode\u003etext\u003c/code\u003e attribute of the Document. After that, the control goes to \u003ccode\u003eCLIPTextEncoder\u003c/code\u003e, which converts the query text into vector embedding.\u003c/li\u003e\n\u003cli\u003eAfter getting the embeddings for the search query, the \u003ccode\u003eSimpleIndexer\u003c/code\u003e compares the query embedding vector with the indexed data to retrieve the top-K nearest neighbours.\u003c/li\u003e\n\u003cli\u003eIn the end, the control goes to the \u003ccode\u003eSimpleRanker\u003c/code\u003e that ranks the results and shows the most relevant ones.\u003c/li\u003e\n\u003c/ul\u003e\n\u003cblockquote\u003e\n\u003cp\u003e\u003cstrong\u003eTips\u003c/strong\u003e: Find more information at Jina Hub about \u003ca href=\"https://hub.jina.ai/executor/livtkbkg\"\u003eCLIPTextEncoder\u003c/a\u003e, \u003ca href=\"https://hub.jina.ai/executor/0hnlmu3q\"\u003eCLIPImageEncoder\u003c/a\u003e and \u003ca href=\"https://hub.jina.ai/executor/zb38xlt4\"\u003eSimpleIndexer\u003c/a\u003e.\u003c/p\u003e\n\u003c/blockquote\u003e\n\u003ch2 id=\"summary\"\u003eSummary\u003c/h2\u003e\n\u003cp\u003eIn this blog, we learned how to create an intelligent in-video visual content search system by leveraging state-of-the-art opensource models with Jina’s framework. This use case can further be extended to incorporate audio data and video frames to improve the quality of search results. We can use the \u003ca href=\"https://github.com/AndreyGuzhov/AudioCLIP\"\u003eAudioCLIP\u003c/a\u003e model from OpenAI to generate embeddings for audio in the same semantic space as images and text.\u003c/p\u003e\n\u003cp\u003eYou can find the application code in the following \u003ca href=\"https://github.com/jina-ai/example-video-search/tree/feat-simple-tutorial\"\u003eGitHub Repository\u003c/a\u003e\u003c/p\u003e\n\u003cp\u003eIn the future posts, we will cover more about building SOTA search applications by leveraging Jina Hub. Stay tuned and happy Searching!\u003c/p\u003e","coverImage":"/assets/images/blog/2021-12-15_in-video_visual_content/banner.png","tags":["video","search"],"sanitize":false}},"__N_SSG":true},"page":"/blog/[slug]","query":{"slug":"2021-12-15_in-video_visual_content"},"buildId":"c5lfS3RD0DKmWm5c4ajJe","runtimeConfig":{},"isFallback":false,"gsp":true,"scriptLoader":[]}</script></body></html>
{"pageProps":{"post":{"title":"Building Open Domain QA system with Jina","date":"2021-12-06","slug":"2021-11-29-odqa-part-1","author":"Nan Wang","content":"<p>Do you struggle to find the right answers to your question from dozens of documents? Do you want to find these answers by asking questions directly in natural languages? Do you get tired of figuring out the proper keywords for the search box? The solution to all of these problems is an intelligent <strong>Question Answering (QA)</strong> system. In this blog, we will discuss how to build the open domain QA system from scratch with Jina.</p>\n<h2 id=\"open-domain-question-answering-odqa-in-2021\">Open Domain Question Answering (ODQA) in 2021</h2>\n<p>Searching for the right information is an integral part of our daily life.</p>\n<p>When you are reading this post, you are likely wondering <em>“What is Jina?”</em>, <em>“How will it help me?\"</em> or maybe <em>“What are the things to know before you start with  Jina?”</em>. The answers to these questions can be easily found in our <a href=\"https://docs.jina.ai/\">documentation</a>. Given such factoid questions asked in natural languages, finding answers based on documents is formatted as <strong>Open Domain Question Answering (ODQA)</strong> in academics.</p>\n<p>A standard ODQA pipeline is a two-stage system containing a <strong>retriever</strong> and <strong>reader</strong>. The retriever retrieves the candidate <strong>contexts</strong> via either the traditional or the neural retrieval methods. The reader extracts answers from the contexts.</p>\n<p>This procedure is the same as how we answer the questions in an open-book examination. Suppose that we have little knowledge of the question but we are allowed to refer to the books during the exam. A common strategy is to firstly find the related chapters or passages and then read through the text to find the exact answer.</p>\n<p class=\"image-only\"><span class=\"image-wrapper\"><img src=\"/assets/images/blog/2021-11-29-odqa-part-1/retriever-reader-pipeline.svg\" alt=\"\"></span></p>\n<p>Let’s now look at different components of this open domain question answering pipeline and how they work in tandem to produce a smooth search interface capable of supporting natural language queries.</p>\n<h2 id=\"retriever\">Retriever</h2>\n<p>Given a bunch of text documents, the retriever selects a few related passages as contexts based on the question. These contexts are later sent to the reader for extracting answers so the reader doesn’t have to read all the documents and search latency will be minimal. Let's look at two different retrieval methods and see how the execution differs for both of them.</p>\n<h3 id=\"term-based-vs-dense-vector-retrieval\">Term-based vs Dense-Vector retrieval</h3>\n<p>Traditionally, the retriever is implemented using <strong>term-based methods</strong>, such as TF-IDF or BM25, which match keywords with an inverted index. This implementation is efficient but suffers from the issue of term mismatching. For example, when you want to know the core concepts in Jina and type <em>“What are the core concepts in Jina?”</em>, you will miss the most important document because the original text is written as <em>“... Document, Executor, and Flow are the three fundamental concepts”</em>. Because the search system does not know <em>“core concepts”</em> are semantically related to <em>“fundamental concepts”</em>.</p>\n<p>Another issue with the term-base method is the expensive query. Questions such as  <em>“What is Jina?”</em> are usually considered expensive queries because such queries will return tons of unrelated results. This is rooted in the fact that the keyword-based search system does not understand the question and purely retrieves all the results containing the keyword <em>“Jina”</em>.</p>\n<p>To address these issues, <strong>dense retrieval methods</strong> have been proposed to replace the term-based method and have been proven to outperform these traditional methods. Instead of building an inverted index and matching the exact keywords, the dense retrieval methods encode the questions and the passages into vectors in a high-dimensional space. The related passages are retrieved by comparing the vector of the question with the vectors of the passages.</p>\n<p class=\"image-only\"><span class=\"image-wrapper\"><img src=\"/assets/images/blog/2021-11-29-odqa-part-1/retriever-comparason.svg\" alt=\"\"></span></p>\n<h3 id=\"encoder\">Encoder</h3>\n<p>To encode the questions and the passages, one option is the widely-used pretrained language models. With Jina Hub, you can directly try out different encoders out-of-box.</p>\n<p>Here we use the <code>TransfomerTorchEncoder</code> which wraps up the <a href=\"https://huggingface.co/transformers/main_classes/model.html\">huggingface transformers library</a> and enables the user to use the pretrained models from huggingface transformers directly. Besides, some of the models from the huggingface model hub are supported as well. Here we use the model from <a href=\"https://huggingface.co/sentence-transformers/all-mpnet-base-v2\">sentence-transformers/all-mpnet-base-v2</a>.</p>\n<pre><code class=\"hljs language-python\"><span class=\"hljs-keyword\">from</span> jina <span class=\"hljs-keyword\">import</span> Executor, Document, DocumentArray\nencoder = Executor.from_hub(<span class=\"hljs-string\">'jinahub://TransformerTorchEncoder'</span>)\nda = DocumentArray([\n    Document(text=<span class=\"hljs-string\">'Jina is a neural search framework.'</span>),\n    Document(text=<span class=\"hljs-string\">'Jina relies heavily on multiprocessing.'</span>),\n    Document(text=<span class=\"hljs-string\">'Jina is backed by Jina AI.'</span>)])\n\nencoder.encode(docs=da)\n<span class=\"hljs-keyword\">for</span> doc <span class=\"hljs-keyword\">in</span> da:\n    <span class=\"hljs-built_in\">print</span>(<span class=\"hljs-string\">f'<span class=\"hljs-subst\">{doc.embedding}</span>'</span>)\n</code></pre>\n<!-- <script src=\"https://gist.github.com/nan-wang/e4a872ec40803eabbbcc6618a4eccb1d.js\"></script> -->\n<blockquote>\n<p><em><strong>Tips</strong>: If you want Jina to automatically install the extra dependencies, create a python virtual environment following the instructions at <a href=\"https://docs.python.org/3/library/venv.html\">virtualenv</a> and change Line 2 to</em></p>\n</blockquote>\n<pre><code class=\"hljs language-diff\"><span class=\"hljs-deletion\">- encoder = Executor.from_hub('jinahub://TransformerTorchEncoder')</span>\n<span class=\"hljs-addition\">+ encoder = Executor.from_hub('jinahub://TransformerTorchEncoder', install_requirements=True)</span>\n</code></pre>\n<p>Notice that we encode the question and the context passages with the same encoder. This might not be ideal because they usually have very different semantic meanings. For example, <em>“What is Jina?”</em> has a very different meaning from <em>“Jina is a neural search framework”</em>.</p>\n<p>A more reasonable approach would be to encode them differently. One of the SOTA models to achieve this is the encoder from <strong>Deep Passage Retrieval (DPR)</strong>, which has trained two BERT models jointly so that the question and the contexts are encoded differently but still into the same space. To choose whether the question or the context model, one can set the <code>encoder_type</code> argument. To try out DPR encoder, you just need to change the following line in the above code</p>\n<pre><code class=\"hljs language-diff\"><span class=\"hljs-deletion\">- encoder = Executor.from_hub('jinahub://TransformerTorchEncoder')</span>\n<span class=\"hljs-addition\">+ encoder = Executor.from_hub('jinahub://DPRTextEncoder', uses_with={'encoder_type': 'context'})</span>\n</code></pre>\n<blockquote>\n<p><em><strong>Tips</strong>: Find more information at Jina Hub about <a href=\"https://hub.jina.ai/executor/u9pqs8eb\">TransformerTorchEncoder</a> and <a href=\"https://hub.jina.ai/executor/awl0jxog\">DPRTextEncoder</a>.</em></p>\n</blockquote>\n<h3 id=\"vector-index\">Vector Index</h3>\n<p>For comparing the vectors and retrieving the <strong><em>top K</em></strong> nearest neighbors to the question vector, we can calculate the cosine similarities for each combination between the question and the passages. The complexity is <em><strong>O(n*log(n))</strong></em> due to the requirement of retrieving top K results.</p>\n<p>Alternatively, we usually resort to the <strong>Approximate Nearest Neighbour (ANN)</strong> algorithms to get an approximated result. In Jina Hub, you can find both implementations easily</p>\n<pre><code class=\"hljs language-python\">indexer = Executor.from_hub(<span class=\"hljs-string\">'jinahub://SimpleIndexer'</span>)\nindexer.index(docs=da)\n\nq_da = DocumentArray([Document(text=<span class=\"hljs-string\">'What is Jina?'</span>)])\n\nindexer.search(docs=q_da)\n<span class=\"hljs-keyword\">for</span> m <span class=\"hljs-keyword\">in</span> q_da[<span class=\"hljs-number\">0</span>].matches:\n    <span class=\"hljs-built_in\">print</span>(<span class=\"hljs-string\">f'score: <span class=\"hljs-subst\">{m.scores[<span class=\"hljs-string\">\"cosine\"</span>].value:<span class=\"hljs-number\">.4</span>f}</span>, text: <span class=\"hljs-subst\">{m.text}</span>'</span>)\n</code></pre>\n<p>To try out the ANN indexer, you just need to change Line 1 as in the below code. Under the hood, we use an indexer based on <a href=\"https://github.com/nmslib/hnswlib\">Hnswlib</a>, which is efficient and flexible in handling large amounts of vectors. The only bottleneck is memory usage because all the vectors are stored in memory in the format of float32.</p>\n<pre><code class=\"hljs language-diff\"><span class=\"hljs-deletion\">- indexer = Executor.from_hub('jinahub://SimpleIndexer')</span>\n<span class=\"hljs-addition\">+ indexer = Executor.from_hub('jinahub://U1MIndexer')</span>\n</code></pre>\n<blockquote>\n<p><em><strong>Tips</strong>: Find more information at Jina Hub about <a href=\"https://hub.jina.ai/executor/zb38xlt4\">SimpleIndexer</a> and <a href=\"https://hub.jina.ai/executor/96v8ecrt\">U1MIndexer</a>.</em></p>\n</blockquote>\n<h2 id=\"reader\">Reader</h2>\n<p>The reader extracts the exact answer from the context. Usually the contexts are long sentences containing multiple factoid and therefore we need the reader to extract the right answer based on the question. This is well studied as the machine reading comprehension problem. Given a question and the candidate contexts, the reader will return a score together with the most possible starting and ending position of the answers.</p>\n<p class=\"image-only\"><span class=\"image-wrapper\"><img src=\"/assets/images/blog/2021-11-29-odqa-part-1/reader.svg\" alt=\"\"></span></p>\n<p>For DPR we will use the pretrained reader model by Facebook Research for the ODQA problem. Under the hood it uses the BERT model to serve two purposes. Firstly, the representation of <code>[CLS]</code> token is used to calculate the relevant scores for each context to measure how relevant they are to the question. This part plays a role as a reranker with a cross-attention mechanism, which has more capacity than the dual encoder model. The downside is that this model is more expensive to compute and therefore it is only feasible to use on a small number of candidates.</p>\n<p>The second usage of the BERT representation is to calculate the probabilities of being a START or END position. Two hidden layers are appended for calculating the probability of being a START or an END position. All the tokens share the same layer weights.</p>\n<p class=\"image-only\"><span class=\"image-wrapper\"><img src=\"/assets/images/blog/2021-11-29-odqa-part-1/dpr_reader.svg\" alt=\"\"></span></p>\n<p><code>DPRReaderRanker</code> is available directly at the <a href=\"https://hub.jina.ai/executor/gzhiwmgg\">Jina Hub</a> as well.</p>\n<pre><code class=\"hljs language-python\">ranker = Executor.from_hub(<span class=\"hljs-string\">'jinahub://DPRReaderRanker'</span>)\nranker.rank(docs=q_da)\n<span class=\"hljs-keyword\">for</span> m <span class=\"hljs-keyword\">in</span> q_da[<span class=\"hljs-number\">0</span>].matches:\n    <span class=\"hljs-built_in\">print</span>(<span class=\"hljs-string\">f'score: <span class=\"hljs-subst\">{m.scores[<span class=\"hljs-string\">\"relevance_score\"</span>].value:<span class=\"hljs-number\">.4</span>f}</span>, text: <span class=\"hljs-subst\">{m.text}</span>'</span>)\n</code></pre>\n<h2 id=\"put-them-all-together-in-a-flow\">Put them all together in a Flow</h2>\n<p>Now we have gone through the components needed for building an ODQA system. The Jina Flow can help us line them together and serve as a service.</p>\n<h3 id=\"index\">Index</h3>\n<p>We first create the Jina Flow and then index all the Documents so that the dense vector retriever can retrieve the context. The Documents are encoded by the <code>DPRTextEncoder</code> and stored by the <code>SimpleIndexer</code>.</p>\n<pre><code class=\"hljs language-python\"><span class=\"hljs-keyword\">from</span> jina <span class=\"hljs-keyword\">import</span> Document, DocumentArray, Flow\n\nda = DocumentArray([\n     Document(text=<span class=\"hljs-string\">'Jina is a neural search framework that empowers anyone to build SOTA and scalable deep learning search applications in minutes.'</span>),\n     Document(text=<span class=\"hljs-string\">'Document, Executor, and Flow are the three fundamental concepts in Jina.'</span>),\n     Document(text=<span class=\"hljs-string\">'Jina is backed by Jina AI and licensed under Apache-2.0.'</span>)])\n\nf = (Flow()\n     .add(uses=<span class=\"hljs-string\">'jinahub+docker://DPRTextEncoder'</span>, encoder_type=<span class=\"hljs-string\">'context'</span>)\n     .add(uses=<span class=\"hljs-string\">'jinahub+docker://SimpleIndexer'</span>))\n\n<span class=\"hljs-keyword\">with</span> f:\n     f.post(on=<span class=\"hljs-string\">'/index'</span>, inputs=da, show_progress=<span class=\"hljs-literal\">True</span>)\n</code></pre>\n<h3 id=\"query\">Query</h3>\n<p>After indexing the Documents, we build a query Flow following the retriever-reader structure.\nWe query with a question in natural languages and get back the answers with relevance scores.</p>\n<pre><code class=\"hljs language-python\"><span class=\"hljs-keyword\">from</span> jina <span class=\"hljs-keyword\">import</span> Flow, Document, DocumentArray\n\nf = (Flow(expose_port=<span class=\"hljs-number\">45678</span>)\n     .add(uses=<span class=\"hljs-string\">'jinahub+docker://DPRTextEncoder'</span>, encoder_type=<span class=\"hljs-string\">'context'</span>)\n     .add(uses=<span class=\"hljs-string\">'jinahub+docker://SimpleIndexer'</span>)\n     .add(uses=<span class=\"hljs-string\">'jinahub+docker://DPRReaderRanker'</span>))\n\nq_da = DocumentArray([Document(text=<span class=\"hljs-string\">'What is Jina?'</span>)])\n\n<span class=\"hljs-keyword\">with</span> f:\n    resp = f.post(on=<span class=\"hljs-string\">'/search'</span>, inputs=q_da, return_results=<span class=\"hljs-literal\">True</span>)\n\n<span class=\"hljs-keyword\">for</span> doc <span class=\"hljs-keyword\">in</span> resp[<span class=\"hljs-number\">0</span>].docs:\n    <span class=\"hljs-built_in\">print</span>(<span class=\"hljs-string\">f'question: <span class=\"hljs-subst\">{doc.text}</span>'</span>)\n    <span class=\"hljs-keyword\">for</span> m <span class=\"hljs-keyword\">in</span> doc.matches:\n        <span class=\"hljs-built_in\">print</span>(<span class=\"hljs-string\">f'score: <span class=\"hljs-subst\">{m.scores[<span class=\"hljs-string\">\"relevance_score\"</span>].value:<span class=\"hljs-number\">.4</span>f}</span>, answer: <span class=\"hljs-subst\">{m.text}</span>'</span>)\n</code></pre>\n<h3 id=\"host-a-restful-service\">Host a RESTful service</h3>\n<p>For hosting the service, we need to change the following line of codes,</p>\n<pre><code class=\"hljs language-diff\"><span class=\"hljs-deletion\">- with f:</span>\n<span class=\"hljs-deletion\">-     f.post(on='/search', inputs=da, return_results=True)</span>\n<span class=\"hljs-addition\">+ with f:</span>\n<span class=\"hljs-addition\">+     f.cors = True</span>\n<span class=\"hljs-addition\">+     f.protocol = 'http'</span>\n<span class=\"hljs-addition\">+     f.block()</span>\n</code></pre>\n<p>Now you can query directly via RESTful API. The Swagger docs UI is available at <code>http://localhost:45678/docs</code></p>\n<pre><code class=\"hljs language-sh\">curl --request POST \\\n     -d <span class=\"hljs-string\">'{\"data\": [\"text\": \"What is Jina?\"]}'</span> \\\n     -H <span class=\"hljs-string\">'Content-Type: application/json'</span> \n     <span class=\"hljs-string\">'http://localhost:45678/search'</span>\n</code></pre>\n<p>A more detailed documentation about the client-serving usage of Jina can be found at <a href=\"https://docs.jina.ai/fundamentals/flow/flow-as-a-service/#flow-as-a-service\">Jina Docs</a></p>\n<h2 id=\"summary\">Summary</h2>\n<p>In this post, we have a gentle walkthrough for building a two-stage ODQA system with Jina. Besides the retriever-reader pipeline, another choice is to replace the reader with a generator for generating answers based on the context.\nFurthermore, with a large language model such as <code>GPT-3</code>, it is also possible to generate answers directly from the question without retrieving any context. However, efficiency and precision are potential issues in practice.</p>\n<p>In the future posts, we will cover more about building ODQA systems with SOTA models in Jina.\nStay tuned and happy Searching!</p>\n<h2 id=\"reference\">Reference</h2>\n<ul>\n<li><a href=\"https://arxiv.org/abs/2004.04906\">Dense Passage Retrieval for Open-Domain Question Answering</a></li>\n<li><a href=\"https://lilianweng.github.io/lil-log/2020/10/29/open-domain-question-answering.html\">Lil'Log</a></li>\n</ul>","coverImage":"/assets/images/blog/2021-11-29-odqa-part-1/banner.jpg","tags":["odqa","qa chatbot"],"sanitize":false}},"__N_SSG":true}